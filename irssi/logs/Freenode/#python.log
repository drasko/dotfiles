--- Log opened Thu Oct 23 00:03:08 2014
00:03 :: Irssi: You are now talking in #python
00:03 :: Irssi: #python: Total of 1552 nicks [0 ops, 0 halfops, 0 voices, 1552 normal]
00:03 :: Irssi: Join to #python was synced in 0 secs
00:03          lemarc : SegFaultAX, how about I use mysql, store the data there and then check for domain in database before processing it so that I can resume from where I left even if the process crashes
00:04      SegFaultAX : lemarc: Sure, any type of database will do.
00:05          lemarc : SegFaultAX, and some more info on the first point please
00:07      SegFaultAX : lemarc: urlparse and urllib? Google is your friend there. But if you want to process URLs in a safer way, Python has this built in. If what you have there is sufficient for all 400k domains, then meh.
00:07      SegFaultAX : lemarc: But you might save some time by normalizing them in a preprocessing step once, so you don't have to do that logic each time.
--- Log closed Thu Oct 23 00:07:25 2014
